{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox9CcHU8g2qW"
      },
      "source": [
        "# **3.1.1 Step -1- Data Understanding, Analysis and Preparations:**\n",
        "# In this step we will read the data, understand the data, perform some basic data cleaning, and store everything in the matrix as shown below.\n",
        "\n",
        "**– Objective of the Task**\n",
        "\n",
        "**To Predict the marks obtained in writing based on the marks of Math and Reading.**\n",
        "\n",
        "• To - Do - 1:\n",
        "\n",
        "1. Read and Observe the Dataset.\n",
        "2. Print top(5) and bottom(5) of the dataset {Hint: pd.head and pd.tail}.\n",
        "3. Print the Information of Datasets. {Hint: pd.info}.\n",
        "4. Gather the Descriptive info about the Dataset. {Hint: pd.describe}\n",
        "5. Split your data into Feature (X) and Label (Y)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxZbGx0JhO61",
        "outputId": "217c270e-ab6c-4f23-f1fd-7238c347aa0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 rows of the dataset:\n",
            "   Math  Reading  Writing\n",
            "0    48       68       63\n",
            "1    62       81       72\n",
            "2    79       80       78\n",
            "3    76       83       79\n",
            "4    59       64       62\n",
            "\n",
            "Bottom 5 rows of the dataset:\n",
            "     Math  Reading  Writing\n",
            "995    72       74       70\n",
            "996    73       86       90\n",
            "997    89       87       94\n",
            "998    83       82       78\n",
            "999    66       66       72\n",
            "\n",
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1000 entries, 0 to 999\n",
            "Data columns (total 3 columns):\n",
            " #   Column   Non-Null Count  Dtype\n",
            "---  ------   --------------  -----\n",
            " 0   Math     1000 non-null   int64\n",
            " 1   Reading  1000 non-null   int64\n",
            " 2   Writing  1000 non-null   int64\n",
            "dtypes: int64(3)\n",
            "memory usage: 23.6 KB\n",
            "\n",
            "Descriptive Statistics of the Dataset:\n",
            "              Math      Reading      Writing\n",
            "count  1000.000000  1000.000000  1000.000000\n",
            "mean     67.290000    69.872000    68.616000\n",
            "std      15.085008    14.657027    15.241287\n",
            "min      13.000000    19.000000    14.000000\n",
            "25%      58.000000    60.750000    58.000000\n",
            "50%      68.000000    70.000000    69.500000\n",
            "75%      78.000000    81.000000    79.000000\n",
            "max     100.000000   100.000000   100.000000\n",
            "\n",
            "Features (X):\n",
            "   Math  Reading\n",
            "0    48       68\n",
            "1    62       81\n",
            "2    79       80\n",
            "3    76       83\n",
            "4    59       64\n",
            "\n",
            "Label (y):\n",
            "0    63\n",
            "1    72\n",
            "2    78\n",
            "3    79\n",
            "4    62\n",
            "Name: Writing, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Loaded the dataset\n",
        "file_path = '/content/drive/MyDrive/Data Set/Copy of student.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Step 2: Printing top(5) and bottom(5) of the dataset\n",
        "print(\"Top 5 rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "print(\"\\nBottom 5 rows of the dataset:\")\n",
        "print(data.tail())\n",
        "\n",
        "# Step 3: Printing the Information of Datasets\n",
        "print(\"\\nDataset Information:\")\n",
        "data.info()\n",
        "\n",
        "# Step 4: Gathering the Descriptive info about the Dataset\n",
        "print(\"\\nDescriptive Statistics of the Dataset:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Step 5: Spliting the data into Feature (X) and Label (Y)\n",
        "\n",
        "X = data[['Math', 'Reading']]\n",
        "y = data['Writing']\n",
        "\n",
        "print(\"\\nFeatures (X):\")\n",
        "print(X.head())\n",
        "\n",
        "print(\"\\nLabel (y):\")\n",
        "print(y.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initializing the weight vector W with random values (same dimension as features in X)\n",
        "W = np.random.rand(X.shape[1])  # W \\in R^d\n",
        "\n",
        "# Printing the initialized weight vector W\n",
        "print(\"\\nWeight vector (W):\")\n",
        "print(W)\n",
        "\n",
        "# Reshape y to ensure it's a column vector\n",
        "Y = np.reshape(y, (-1, 1))  # Y \\in R^n\n",
        "\n",
        "# Printing the target vector Y\n",
        "print(\"\\nTarget vector (Y):\")\n",
        "print(Y[:5])\n",
        "\n",
        "# Verifying the dimensions\n",
        "print(\"\\nDimensions:\")\n",
        "print(f\"W: {W.shape}, X: {X.shape}, Y: {Y.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-NKP1gc2IH_",
        "outputId": "fe64883c-ecee-49fa-a07d-ba5cda0e52f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Weight vector (W):\n",
            "[0.4822373  0.58066864]\n",
            "\n",
            "Target vector (Y):\n",
            "[[63]\n",
            " [72]\n",
            " [78]\n",
            " [79]\n",
            " [62]]\n",
            "\n",
            "Dimensions:\n",
            "W: (2,), X: (1000, 2), Y: (1000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# • To - Do - 3:\n",
        "**1. Split the dataset into training and test sets.**\n",
        "\n",
        "**2. You can use an 80-20 or 70-30 split, with 80% (or 70%) of the data used for training and the rest for testing.**"
      ],
      "metadata": {
        "id": "Je-8VkSA2RFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#spliting the dataset\n",
        "split_ratio = 0.8  # 80-20 split\n",
        "split_index = int(len(X) * split_ratio)\n",
        "\n",
        "# Shuffling the data\n",
        "indices = np.arange(len(X))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Converting to NumPy arrays (if not already)\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Applying the shuffled indices\n",
        "X_shuffled = X[indices]\n",
        "y_shuffled = y[indices]\n",
        "\n",
        "# Spliting the data\n",
        "X_train = X_shuffled[:split_index]\n",
        "X_test = X_shuffled[split_index:]\n",
        "y_train = y_shuffled[:split_index]\n",
        "y_test = y_shuffled[split_index:]\n",
        "\n",
        "print(\"\\nTraining Features (X_train):\")\n",
        "print(X_train[:5])\n",
        "\n",
        "print(\"\\nTesting Features (X_test):\")\n",
        "print(X_test[:5])\n",
        "\n",
        "print(\"\\nTraining Labels (y_train):\")\n",
        "print(y_train[:5])\n",
        "\n",
        "print(\"\\nTesting Labels (y_test):\")\n",
        "print(y_test[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2GUYcM22bpm",
        "outputId": "ac0417e2-9ad7-491f-f6fe-fef27411d01e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Features (X_train):\n",
            "[[77 79]\n",
            " [70 67]\n",
            " [77 85]\n",
            " [85 82]\n",
            " [54 51]]\n",
            "\n",
            "Testing Features (X_test):\n",
            "[[73 64]\n",
            " [56 65]\n",
            " [44 55]\n",
            " [70 81]\n",
            " [71 64]]\n",
            "\n",
            "Training Labels (y_train):\n",
            "[84 64 81 83 53]\n",
            "\n",
            "Testing Labels (y_test):\n",
            "[69 66 66 78 58]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.1.2 Step -2- Build a Cost Function:**\n",
        "# Cost function is the average of loss function measured across the data point."
      ],
      "metadata": {
        "id": "Dbr0Tv1I2lEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    X: Feature Matrix\n",
        "    Y: Target Matrix\n",
        "    W: Weight Matrix\n",
        "\n",
        "    Returns:\n",
        "    cost: accumulated mean square error.\n",
        "    \"\"\"\n",
        "    # Hypothesis function\n",
        "    y_pred = np.dot(X, W)\n",
        "\n",
        "    # Mean Squared Error calculation\n",
        "    errors = y_pred - Y.flatten()\n",
        "    cost = (1 / (2 * len(Y))) * np.sum(errors ** 2)\n",
        "    return cost"
      ],
      "metadata": {
        "id": "cB1mnmhL2nOk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Designing a Test Case for Cost Function:\n",
        "**We will first calculate the loss value manually and then verify the output via our code. If the computed valuematches, we will proceed further**"
      ],
      "metadata": {
        "id": "49OH9E6Z2sCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test case for Cost Function as given in the question\n",
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "\n",
        "# Calculating the cost function\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "if cost == 0:\n",
        "    print(\"\\nProceed Further\")\n",
        "else:\n",
        "    print(\"\\nSomething went wrong: Reimplement the cost function\")\n",
        "\n",
        "print(\"Cost function output:\", cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqkzzFiR2t05",
        "outputId": "40ebbf73-658b-45ed-93c0-53c29d37ab95"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Proceed Further\n",
            "Cost function output: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.1.3 Step -3- Gradient Descent for Simple Linear Regression:**\n",
        "# Objective: Learn the Parameters\n",
        "**To learn the parameters w (weights) and b (biases), we will assume that b = 0 for simplicity. Thus no need to update biases or w0.**\n",
        "\n",
        "# ***Gradient Descent Code below***"
      ],
      "metadata": {
        "id": "6xeF8-7d2zf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing Gradient Descent function\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (m x n).\n",
        "    Y (numpy.ndarray): Target vector (m x 1).\n",
        "    W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "    alpha (float): Learning rate.\n",
        "    iterations (int): Number of iterations for gradient descent.\n",
        "    Returns:\n",
        "    tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values.\n",
        "    W_update (numpy.ndarray): Updated parameters (n x 1).\n",
        "    cost_history (list): History of cost values over iterations.\n",
        "    \"\"\"\n",
        "    # Initializing cost history\n",
        "    cost_history = [0] * iterations\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    W_update = W.copy()\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values\n",
        "        Y_pred = np.dot(X, W_update)\n",
        "\n",
        "        # Step 2: Difference between Hypothesis and Actual Y\n",
        "        loss = Y_pred - Y.flatten()\n",
        "\n",
        "        # Step 3: Gradient Calculation\n",
        "        dw = (1 / m) * np.dot(X.T, loss)\n",
        "\n",
        "        # Step 4: Updating Values of W using Gradient\n",
        "        W_update = W_update - alpha * dw\n",
        "\n",
        "        # Step 5: New Cost Value\n",
        "        cost = cost_function(X, Y, W_update)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "    return W_update, cost_history"
      ],
      "metadata": {
        "id": "YwhLLV5822Nf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Test Code for Gradient Descent function below:***"
      ],
      "metadata": {
        "id": "h-ZgJ6dv26VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the gradient_descent function\n",
        "np.random.seed(0)  # For reproducibility\n",
        "X_test = np.random.rand(100, 3)  # 100 samples, 3 features\n",
        "Y_test = np.random.rand(100)\n",
        "W_test = np.random.rand(3)  # Initial guess for parameters\n",
        "\n",
        "# Set hyperparameters\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "# Perform Gradient Descent\n",
        "final_params, cost_history = gradient_descent(X_test, Y_test, W_test, alpha, iterations)\n",
        "\n",
        "# Print the final parameters and cost history\n",
        "print(\"\\nFinal Parameters:\", final_params)\n",
        "print(\"\\nCost History:\", cost_history[:10], \"...\")  # Display first 10 cost values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UC_jWN828fP",
        "outputId": "598211a6-9d02-4f27-c019-6cefa1aad834"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Parameters: [0.20551667 0.54295081 0.10388027]\n",
            "\n",
            "Cost History: [0.10711197094660153, 0.10634880599939901, 0.10559826315680618, 0.10486012948320558, 0.1041341956428534, 0.10342025583900626, 0.1027181077540776, 0.1020275524908062, 0.10134839451441931, 0.1006804415957737] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.1.4 Step -4- Evaluate the Model:**\n",
        "**Evaluation in Machine Learning measures the goodness of fit of your build model. Lets see How Good ismodel we designed above, as discussed in the class for regression we can use following function as evaluation**\n",
        "**measure.**\n",
        "# ***1. Root Mean Square Error:***\n",
        "***The Root Mean Squared Error (RMSE) is a commonly used metric for measuring the average magnitude of the errors between predicted and actual values.**\n",
        "\n",
        "# **The code for RMSE Model is below**"
      ],
      "metadata": {
        "id": "yhle2LlT3BUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IVd-Nb2U3Ahi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation For RMSE\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the Root Mean Squares.\n",
        "    Input Arguments:\n",
        "    Y: Array of actual (Target) Dependent Variables.\n",
        "    Y_pred: Array of predicted Dependent Variables.\n",
        "    Output Arguments:\n",
        "    rmse: Root Mean Square.\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.mean((Y - Y_pred) ** 2))"
      ],
      "metadata": {
        "id": "G9s9TDH63D46"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***R2 or Coefficient of Determination:***\n",
        "***R-squared, or the coefficient of determination, measures the proportion of the variance in the dependent variable that is predictable from the independent variables.***\n",
        "# **The code for R2 Model is below**"
      ],
      "metadata": {
        "id": "AMPNmeB_3Hse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation For R2\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the R Squared Error.\n",
        "    Input Arguments:\n",
        "    Y: Array of actual (Target) Dependent Variables.\n",
        "    Y_pred: Array of predicted Dependent Variables.\n",
        "    Output Arguments:\n",
        "    r2: R Squared Error.\n",
        "    \"\"\"\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    return 1 - (ss_res / ss_tot)"
      ],
      "metadata": {
        "id": "_dGKgEmu3RjL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.1.5 Step -5- Main Function to Integrate All Steps:**\n",
        "**In this section, we will create a main function that integrates the data loading, preprocessing, cost function,gradient descent, and model evaluation. This will help in running the entire workflow with minimal effort.**\n",
        "\n",
        "# • Objective:\n",
        "**The objective of the main function is to execute the full process, from loading the data to performinglinear regression using gradient descent and evaluating the results using metrics like RMSE and R2**\n",
        ".\n",
        "\n",
        "**• To - Do:**\n",
        "\n",
        "We will define a function that:\n",
        "\n",
        "1. Loads the data and splits it into training and test sets.\n",
        "2. Prepares the feature matrix (X) and target vector (Y).\n",
        "3. Defines the weight matrix (W) and initializes the learning rate and number of iterations.\n",
        "4. Calls the gradient descent function to learn the parameters.\n",
        "5. Evaluates the model using RMSE and R2.\n",
        "Re-wrote the following code below"
      ],
      "metadata": {
        "id": "6v5Q4-9f3XPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv('/content/drive/MyDrive/Data Set/Copy of student.csv')\n",
        "\n",
        "    # Step 2: Split the data into features (X) and target (Y)\n",
        "    X = data[['Math', 'Reading']].values  # Features: Math and Reading marks\n",
        "    Y = data['Writing'].values            # Target: Writing marks\n",
        "\n",
        "    # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Step 4: Initialize weights (W) to zeros, learning rate, and number of iterations\n",
        "    W = np.zeros(X_train.shape[1])  # Initialize weights\n",
        "    alpha = 0.00001  # Learning rate\n",
        "    iterations = 1000  # Number of iterations for gradient descent\n",
        "\n",
        "    # Step 5: Perform Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    # Step 6: Make predictions on the test set\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Output the results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1IHnRui3hhq",
        "outputId": "7335f1ee-89d4-4737-fb10-6d4fa5e3efec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.34811659 0.64614558]\n",
            "Cost History (First 10 iterations): [2013.165570783755, 1640.286832599692, 1337.0619994901588, 1090.4794892850578, 889.9583270083234, 726.8940993009545, 594.2897260808594, 486.4552052951635, 398.7634463599484, 327.4517147324688]\n",
            "RMSE on Test Set: 5.2798239764188635\n",
            "R-Squared on Test Set: 0.8886354462786421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Present your finding:\n",
        "# **1. Did your Model Overfitt, Underfitts, or performance is acceptable?**\n",
        "\n",
        "# **Ans:-**\n",
        "**The results show that the R-squared value on the test set is 0.88863, indicating that the model explains 88.9% of the variance in the data, which is generally a strong performance.**\n",
        "\n",
        "**The first 10 values of the cost function history demonstrate a decrease from 2013.16 to 327.45, signifying that the model is learning from its errors and improving over time.**\n",
        "\n",
        "**Overall, the model performs well, capturing a significant portion of the data's variance. The RMSE value is relatively low, and the decreasing cost history further confirms that the model is making fewer errors and improving. These metrics suggest that the model is neither overfitting nor underfitting, and its performance is satisfactory.**"
      ],
      "metadata": {
        "id": "FBjlEXMP3syd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experimenting with different learning rates\n",
        "alpha_values = [0.0001, 0.0005, 0.001, 0.01]  # Example learning rates\n",
        "\n",
        "for alpha in alpha_values:\n",
        "    print(f\"\\nTesting with alpha = {alpha}\")\n",
        "    W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "    print(\"RMSE on Test Set:\", model_rmse)\n",
        "    print(\"R-Squared on Test Set:\", model_r2)"
      ],
      "metadata": {
        "id": "b3eozyhL3_A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Experiment with different value of learning rate, making it higher and lower, observe the result.**\n",
        "\n",
        "# **Ans:-**\n",
        "\n",
        "**1. When alpha is set too high (e.g., 0.01), the model may fail to converge properly, causing the cost to oscillate or even increase.**\n",
        "\n",
        "\n",
        "**2. When alpha is too low (e.g., 0.0001), the model may converge very slowly and may not achieve a satisfactory solution within a reasonable number of iterations.**\n",
        "\n",
        "\n",
        "**3. The optimal alpha ensures steady convergence, with a consistently decreasing cost and acceptable RMSE and R-squared values. Adjusting alpha based on the model's performance is essential for achieving the best results.**"
      ],
      "metadata": {
        "id": "jK1a_0mE4FEv"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}